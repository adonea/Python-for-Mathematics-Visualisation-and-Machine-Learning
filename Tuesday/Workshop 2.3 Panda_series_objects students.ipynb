{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "It contains high-level data structures and manipulation tools designed to make data analysis fast and easy in Python. pandas is built on top of NumPy and makes it easy to use in NumPy-centric applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data reading and manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Machine learning tools\n",
    "import sklearn\n",
    "from sklearn import (metrics, model_selection, feature_selection,\n",
    "                     preprocessing, naive_bayes, linear_model)\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "print(f\"Numpy:        {np.__version__}\")\n",
    "print(f\"Pandas:       {pd.__version__} <- ensure this is 2.0 or higher\")\n",
    "print(f\"Matplotlib:   {matplotlib.__version__}\")\n",
    "print(f\"Seaborn:      {sns.__version__}\")\n",
    "print(f\"Scikit-learn: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART1 Data Structures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with pandas, you will need to get comfortable with its two workhorse data structures: Series and DataFrame. While they are not a universal solution for every problem, they provide a solid, easy-to-use basis for most applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Series is a one-dimensional array-like object containing an array of data (of any NumPy data type) and an associated array of data labels, called its index. The simplest Series is formed from only an array of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Series([4, 7, -5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the numerical values only\n",
    "\n",
    "obj.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read index \n",
    "obj.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it will be desirable to create a Series with an index identifying each data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2 = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the attribute of a\n",
    "\n",
    "obj2[?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change an element: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2['d'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2[['c', 'a', 'd']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtering with a boolean array, scalar multiplication, or applying math functions, will preserve the index-value link:\n",
    "\n",
    "choose only values in obj2 > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2[?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply all values by a factor 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(obj2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New data: create a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj3 = Series(sdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame represents a tabular, spreadsheet-like data structure containing an ordered collection of columns, each of which can be a different value type (numeric, string, boolean, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], 'year': [2000, 2001, 2002, 2001, 2002],\n",
    "'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\n",
    "frame = DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting DataFrame will have its index assigned automatically as with Series, and the columns are placed in sorted order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order of columns is specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(data, columns=['year', 'state', 'pop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Change the order of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(data, columns=[?,?,? ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Series, if you pass a column that isn’t contained in data, it will appear with NA values in the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame2 = DataFrame(data, columns=['year', 'state', 'pop', 'debt'], \n",
    "                   index=['one', 'two', 'three', 'four', 'five'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the column from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame3=frame2.drop(columns=[?])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common form of data is a nested dict of dicts format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = {'Nevada': {2001: 2.4, 2002: 2.9},\n",
    "       'Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame3 = DataFrame(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data is common in most data analysis applications. One of the goals in de- signing pandas was to make working with missing data as painless as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_data = Series(['aardvark', 'artichoke', np.nan, 'avocado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas uses the floating point value NaN (Not a Number) to represent missing data in both floating as well as in non-floating point arrays. It is just used as a sentinel that can be easily detected:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Out Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a number of options for filtering out missing data. While doing it by hand is always an option, dropna can be very helpful. On a Series, it returns the Series with only the non-null data and index values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan as NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Series([1, NA, 3.5, NA, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, you could have computed this yourself by boolean indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With DataFrame objects, these are a bit more complex. You may want to drop rows or columns which are all NA or just those containing any NAs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame([[1., 6.5, 3.], [1., NA, NA], \n",
    "                  [NA, NA, NA], [NA, 6.5, 3.]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropna by default drops any row containing a missing value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing how='all' will only drop rows that are all NA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns in the same way is only a matter of passing axis=1:\n",
    "\n",
    "Generate a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[4]=NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related way to filter out DataFrame rows tends to concern time series data. Suppose you want to keep only rows containing a certain number of observations. You can indicate this with the thresh argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(np.random.randn(7, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ix[:4, 1] = NA; df.ix[:2, 2] = NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(thresh=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling in Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than filtering out missing data (and potentially discarding other data along with it), you may want to fill in the “holes” in any number of ways. For most purposes, the fillna method is the workhorse function to use. Calling fillna with a constant replaces missing values with that value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna({1: 0.5, 3: -1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fillna returns a new object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation is a statistical technique that shows how two variables are related. Pandas dataframe.corr() method is used for creating the correlation matrix. It is used to find the pairwise correlation of all columns in the dataframe. Any na values are automatically excluded. For any non-numeric data type columns in the dataframe it is ignored.\n",
    "To create correlation matrix using pandas, these steps should be taken: \n",
    " \n",
    "\n",
    "- Obtain the data.\n",
    "- Create the DataFrame using Pandas.\n",
    "- Create correlation matrix using Pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    " \n",
    "# obtaining the data\n",
    "data = {'A': [45, 37, 42],\n",
    "        'B': [38, 31, 26],\n",
    "        'C': [10, 15, 17]\n",
    "        }\n",
    "# creation of DataFrame\n",
    "df = pd.DataFrame(data)\n",
    " \n",
    "# creation of correlation matrix\n",
    "corrM = df.corr()\n",
    " \n",
    "corrM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE: Drop C and recallculate the corrM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize the correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your main goal is to visualize the correlation matrix, rather than creating a plot per se, the convenient pandas styling options is a viable built-in solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rs = np.random.RandomState(0)\n",
    "df = pd.DataFrame(?.rand(10, 10))\n",
    "corr = ?.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')\n",
    "# 'RdBu_r', 'BrBG_r', & PuOr_r are other good diverging colormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,8))\n",
    "sns.heatmap(corr, cmap=\"Greens\",annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single corner heatmap\n",
    "\n",
    "how to only show one corner of the correlation matrix. I find this easier to read myself, since it removes the redundant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill diagonal and upper half with NaNs\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "corr[mask] = np.nan\n",
    "(corr\n",
    " .style\n",
    " .background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1)\n",
    " .highlight_null(color='#f1f1f1')  # Color NaNs grey\n",
    " .format(precision=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with correlations between a large number of features I find it useful to cluster related features together. This can be done with the seaborn clustermap plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.clustermap(?.corr(), \n",
    "                   method = 'complete', \n",
    "                   cmap   = 'RdBu', \n",
    "                   annot  = True, \n",
    "                   annot_kws = {'size': 8})\n",
    "plt.setp(g.ax_heatmap.get_xticklabels(), rotation=60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "\n",
    "It is a table that is used in classification problems to assess where errors in the model were made.\n",
    "\n",
    "The y-data  represent the actual classes the outcomes should have been. While the y-predicted represent the predictions we have made. Using this table it is easy to see which predictions are wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'y_actual':    [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
    "        'y_predicted': [1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0]\n",
    "        }\n",
    "\n",
    "dfA = pd.DataFrame(data)\n",
    "print(dfA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create the Confusion Matrix using pandas, you’ll need to apply the pd.crosstab as follows:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = {'y_actual':    [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
    "        'y_predicted': [1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0]\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "confusion_matrix = pd.crosstab(df['y_actual'], df['y_predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the Confusion Matrix using seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {'y_actual':    [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
    "        'y_predicted': [1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0]\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "confusion_matrix = pd.crosstab(df['y_actual'], df['y_predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "sn.heatmap(confusion_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Results Explained\n",
    "\n",
    "The Confusion Matrix created has four different quadrants:\n",
    "\n",
    "- True Negative (Top-Left Quadrant)\n",
    "- False Positive (Top-Right Quadrant)\n",
    "- False Negative (Bottom-Left Quadrant)\n",
    "- True Positive (Bottom-Right Quadrant)\n",
    "- True means that the values were accurately predicted, False means that there was an error or wrong prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://www.w3schools.com/python/python_ml_confusion_matrix.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:yellow;color:black\">Let's do serious work</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Let's do  serious work with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is work presented by Paul Hancock as the 2023 ASa Machine Learning Workshop.This work is credited to him and his team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be using a larger data set this time, and one which has only galaxies in it.\n",
    "You can download and view the data set from this [link](https://raw.githubusercontent.com/PaulHancock/2023-ASA-ML-DeepDive/main/SDSS_10k_Galaxy.csv).\n",
    "We'll download it directly to our colaboratory using `wget` (if you have it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!  wget https://raw.githubusercontent.com/PaulHancock/2023-ASA-ML-DeepDive/main/SDSS_10k_Galaxy.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the data set and inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy_df = pd.read_csv(\"SDSS_10k_Galaxy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Find the size of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a summary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scale\n",
    "\n",
    "Note that the columns have a different mean, std, and min/max.\n",
    "Also note that some of the columns are either empty (all `NaN`) or information free (constant values).\n",
    "\n",
    "The regression algorithims that we will use today are distance based methods, which means that they are sensitive to differences in the scale of our features.\n",
    "For example, we have $\\alpha ∈ (-10, +65)$, the $\\textrm{objid} ∈ (1e+18,2e+18)$, and $\\textrm{redshift} ∈ (-0.001, 2)$.\n",
    "A \"distance\" of 1 in each of these dimensions is not equally relevant.\n",
    "\n",
    "If we plot a histogram for each feature we can see the different scales an distributions of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a grid of axes ahead of time, otherwise pandas tries to put these all on the same plot\n",
    "fig, ax = plt.subplots(3,7, figsize=(21, 7))\n",
    "galaxy_df.hist(grid=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to eliminate issues with scaling we will transform all our numeric features using a min/max scaler. Some of the features look like they may have a normal or power-law distribution, so min/max might not be the best solution, but this is what we'll go with for now. You can come back and try different scalers at a later time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing classes have a similar interface to the ML models:\n",
    "- `.fit` to fit the transformer to your data\n",
    "- `.transform` to return the transformed data\n",
    "- `.fit_transform` to do both at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select just the numeric data, copy it because we will modify it\n",
    "numerics = galaxy_df.select_dtypes(include=np.number)\n",
    "# create a minmax scaler\n",
    "minmax = preprocessing.MinMaxScaler()\n",
    "# \"fit\" the scaler -> determine the min/max for each column\n",
    "minmax.fit(numerics)\n",
    "\n",
    "# Transform the data and save it back to our dataframe\n",
    "numerics[:] = minmax.transform(numerics)\n",
    "#        ^ this slicing hack causes the values of the pandas dataframe to\n",
    "# be overwritten with the values from a numpy array (from the tranform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at our scaled data to check the effects of our code.\n",
    "\n",
    "You should see all columns have a  𝑚𝑖𝑛=0  and  𝑚𝑎𝑥=1.0  (except for columns which had constant values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo the plot\n",
    "fig, ax = plt.subplots(3,7, figsize=(21, 7))\n",
    "numerics.hist(grid=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more compact way to visualise the spread of data within our range $[0,1]$ is to use a box and whisker plot.\n",
    "\n",
    "Seaborn gives us a fairly nice way to do this, we just have to make sure that we fiddle the axis labels so they don't overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(numerics)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for missing data (rows/columns)\n",
    "\n",
    "Most ML models don't know what to do with missing values, and with either complain or (worse) silently give bad results.\n",
    "\n",
    "If we find missing data then we should consider the following actions:\n",
    "- remeasure the value\n",
    "- impute a value (compute using the rest of the data)\n",
    "- delete the row with missing data\n",
    "- delete the column with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        |--> ask if any of the values are nul/nan, returns a new data frame of true/false\n",
    "numerics.isna().any()\n",
    "#              |--> ask if any of the columns contain a true value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpType: misses values. Let's get rid of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics=numerics.drop(columns='SpType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numerics.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=['specobjid','plate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our target column\n",
    "y = numerics['redshift']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection'\n",
    "\n",
    "We now want to start removing features that we don't think are useful. By \"useful\" we mean that they could help with our regression task, which in this case is predicting the redshifts.\n",
    "\n",
    "There are a few ways to do this:\n",
    "\n",
    "- If you have domain knowledge then you can elliminate features a priori (eg. objid should be unrelated to any physical features of a galaxy)\n",
    "- You can remove features with low variance. The assumption being that low variance means that the feature is unlikely to have much predictive power. (lower variance means more likely to be constant).\n",
    "- You can look at how well each feature correlates with the target (redshift) and keep only features with high correlation. This assumes a linear realationship between features (but then so do many ML models!)\n",
    "- You can recursively apply a ML model to different subsets of features and see how well they can predict the target and then eliminate those that do the worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kb_features(X,y,k):\n",
    "  \"\"\"\n",
    "  Select the k best features based on the mutual information metric\n",
    "\n",
    "  parameters\n",
    "  ----------\n",
    "  X : pandas.DataFrame\n",
    "    Features for prediction\n",
    "\n",
    "  y : pandas.DataFrame\n",
    "    Target features that will be predicted\n",
    "\n",
    "  k : int\n",
    "    The number of best features to keep\n",
    "\n",
    "  returns\n",
    "  -------\n",
    "  selected : list\n",
    "    A list of the k best features that were selected. The ordering\n",
    "    matches the ordering within the data set X, not the order of 'best-ness'\n",
    "  \"\"\"\n",
    "\n",
    "  # create a feature selection model\n",
    "  kb = feature_selection.SelectKBest(feature_selection.mutual_info_regression, k=k)\n",
    "\n",
    "  # fit the model and get the selected features\n",
    "  kb.fit(X,y)\n",
    "  kb_selected = kb.get_feature_names_out()\n",
    "\n",
    "  # make some summary output\n",
    "  for feature,score in zip(X.columns, kb.scores_):\n",
    "    print(f\"{feature:9s} = {score:5.3f}\", end='')\n",
    "    if feature in kb_selected:\n",
    "      print(\" <- selected\")\n",
    "    else:\n",
    "      print()\n",
    "\n",
    "  # return the selected features\n",
    "  return kb_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now 8:select the best features\n",
    "selected_features = kb_features(X,y,8)\n",
    "\n",
    "print(f\"Selected features: {selected_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What do you notice about the features being selected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation between each of the numeric parameters as well as the\n",
    "# target attribute\n",
    "cor = X.join(y).corr()\n",
    "\n",
    "# use seaborn to do the plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(24,24))\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee59c53c",
   "metadata": {},
   "source": [
    "# In Depth: Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627b60f",
   "metadata": {},
   "source": [
    "Here we will move on to another class of unsupervised machine learning models: clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7209d8b4",
   "metadata": {},
   "source": [
    "The k-means clustering model  is simple and relatively easy to understand, but its simplicity leads to practical challenges in its application. In particular, the non-probabilistic nature of k-means and its use of simple distance-from-cluster-center to assign cluster membership leads to poor performance for many real-world situations. \n",
    "\n",
    "\n",
    "In this section we will take a look at Gaussian mixture models (GMMs), which can be viewed as an extension of the ideas behind k-means, but can also be a powerful tool for estimation beyond simple clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767a4c6",
   "metadata": {},
   "source": [
    "A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9dee2",
   "metadata": {},
   "source": [
    "# Examples of the different methods of initialization in Gaussian Mixture Models\n",
    "\n",
    "Introducing k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4546344",
   "metadata": {},
   "source": [
    "Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is an algorithm known as k-means clustering, which is implemented in sklearn.cluster.KMeans.\n",
    "\n",
    "The k-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset. It accomplishes this using a simple conception of what the optimal clustering looks like:\n",
    "\n",
    "- 1.The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n",
    "- 2.Each point is closer to its own cluster center than to other cluster centers.\n",
    "\n",
    "\n",
    "Those two assumptions are the basis of the k-means model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.utils.extmath import row_norms\n",
    "from sklearn.datasets._samples_generator import make_blobs\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import seaborn as sns; sns.set()  # for plot styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748f8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f145f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "\n",
    "X, y_true = make_blobs(n_samples=400, centers=4, cluster_std=0.50, random_state=0)\n",
    "X = X[:, ::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde7a47f",
   "metadata": {},
   "source": [
    "Let's take a look at some of the weaknesses of k-means and think about how we might improve the cluster model. \n",
    "\n",
    "\n",
    "Weakness: A common challenge with k-means is that you must tell it how many clusters you expect: it cannot learn the number of clusters from the data. For example, if we ask the algorithm to identify six clusters, it will happily proceed and find the best six clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data with K Means Labels\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(4, random_state=0) #kmeans = KMeans(n_clusters=4)\n",
    "labels = kmeans.fit(X).predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis');\n",
    "# add cluster centres :-)\n",
    "plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],color = \"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fbf5e1",
   "metadata": {},
   "source": [
    "By eye, it is relatively easy to pick out the four clusters and the centres. The k-means algorithm does this automatically. But how? the typical approach  involves an intuitive iterative approach known as expectation–maximization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6d703",
   "metadata": {},
   "source": [
    "Note: From an intuitive standpoint, we might expect that the clustering assignment for some points is more certain than others: for example, there appears to be a very slight overlap between the two middle clusters, such that we might not have complete confidence in the cluster assigment of points between them. Unfortunately, the k-means model has no intrinsic measure of probability or uncertainty of cluster assignments (although it may be possible to use a bootstrap approach to estimate this uncertainty). For this, we must think about generalizing the model.\n",
    "\n",
    "One way to think about the k-means model is that it places a circle (or, in higher dimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the most distant point in the cluster. This radius acts as a hard cutoff for cluster assignment within the training set: any point outside this circle is not considered a member of the cluster. We can visualize this cluster model with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b652ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):\n",
    "    labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # plot the input data\n",
    "    ax = ax or plt.gca()\n",
    "    ax.axis('equal')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "\n",
    "    # plot the representation of the KMeans model\n",
    "    centers = kmeans.cluster_centers_\n",
    "    radii = [cdist(X[labels == i], [center]).max()\n",
    "             for i, center in enumerate(centers)]\n",
    "    for c, r in zip(centers, radii):\n",
    "        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "plot_kmeans(kmeans, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3af54",
   "metadata": {},
   "source": [
    "Limitation:\n",
    "    \n",
    "    k-means is limited to linear cluster boundaries\n",
    "The fundamental model assumptions of k-means (points will be closer to their own cluster center than to others) means that the algorithm will often be ineffective if the clusters have complicated geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(200, noise=.05, random_state=0)\n",
    "labels = KMeans(2, random_state=0).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEt's classify this: assigns labels using a k-means algorithm (more later)\n",
    "# https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eae6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',\n",
    "                           assign_labels='kmeans')\n",
    "labels = model.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adf3b2",
   "metadata": {},
   "source": [
    "# k-Means Algorithm: Expectation–Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff1095",
   "metadata": {},
   "source": [
    "Expectation–maximization (E–M) is a powerful algorithm that comes up in a variety of contexts within data science. k-means is a particularly simple and easy-to-understand application of the algorithm, and we will walk through it briefly here. In short, the expectation–maximization approach here consists of the following procedure:\n",
    "\n",
    "1.Guess some cluster centers\n",
    "\n",
    "2.Repeat until converged\n",
    "\n",
    "E-Step: assign points to the nearest cluster center\n",
    "M-Step: set the cluster centers to the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4458804a",
   "metadata": {},
   "source": [
    "\n",
    "The four initializations are kmeans (default), random, random_from_data and k-means++.\n",
    "\n",
    "Orange diamonds represent the initialization centers for the gmm generated by the init_param. The rest of the data is represented as crosses and the colouring represents the eventual associated classification after the GMM has finished.\n",
    "\n",
    "The numbers in the top right of each subplot represent the number of iterations taken for the GaussianMixture to converge and the relative time taken for the initialization part of the algorithm to run. The shorter initialization times tend to have a greater number of iterations to converge.\n",
    "\n",
    "The initialization time is the ratio of the time taken for that method versus the time taken for the default kmeans method. As you can see all three alternative methods take less time to initialize when compared to kmeans.\n",
    "\n",
    "In this example, when initialized with random_from_data or random the model takes more iterations to converge. Here k-means++ does a good job of both low time to initialize and low number of GaussianMixture iterations to converge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b2df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 4000\n",
    "n_components = 3\n",
    "x_squared_norms = row_norms(X, squared=True)\n",
    "\n",
    "\n",
    "def get_initial_means(X, init_params, r):\n",
    "    # Run a GaussianMixture with max_iter=0 to output the initalization means\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=3, init_params=init_params, tol=1e-9, max_iter=0, random_state=r\n",
    "    ).fit(X)\n",
    "    return gmm.means_\n",
    "\n",
    "\n",
    "methods = [\"kmeans\", \"random_from_data\", \"k-means++\", \"random\"]\n",
    "colors = [\"navy\", \"turquoise\", \"cornflowerblue\", \"darkorange\"]\n",
    "times_init = {}\n",
    "relative_times = {}\n",
    "\n",
    "plt.figure(figsize=(4 * len(methods) // 2, 6))\n",
    "plt.subplots_adjust(\n",
    "    bottom=0.1, top=0.9, hspace=0.15, wspace=0.05, left=0.05, right=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7391183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, method in enumerate(methods):\n",
    "    r = np.random.RandomState(seed=1234)\n",
    "    plt.subplot(2, len(methods) // 2, n + 1)\n",
    "\n",
    "    start = timer()\n",
    "    ini = get_initial_means(X, method, r)\n",
    "    end = timer()\n",
    "    init_time = end - start\n",
    "\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=3, means_init=ini, tol=1e-9, max_iter=2000, random_state=r\n",
    "    ).fit(X)\n",
    "\n",
    "    times_init[method] = init_time\n",
    "    for i, color in enumerate(colors):\n",
    "        data = X[gmm.predict(X) == i]\n",
    "        plt.scatter(data[:, 0], data[:, 1], color=color, marker=\"x\")\n",
    "\n",
    "    plt.scatter(\n",
    "        ini[:, 0], ini[:, 1], s=55, marker=\"D\", c=\"orange\", lw=1.5, edgecolors=\"black\"\n",
    "    )\n",
    "    relative_times[method] = times_init[method] / times_init[methods[0]]\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(method, loc=\"left\", fontsize=12)\n",
    "    plt.title(\n",
    "        \"Iter %i | Init Time %.2fx\" % (gmm.n_iter_, relative_times[method]),\n",
    "        loc=\"right\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "plt.suptitle(\"GMM iterations and relative time taken to initialize\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ca6348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9df070de",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4026f",
   "metadata": {},
   "source": [
    "Here we will attempt to use k-means to try to identify similar digits without using the original label information; this might be similar to a first step in extracting meaning from a new dataset about which you don't have any a priori label information.\n",
    "\n",
    "We will start by loading the digits and then finding the KMeans clusters. Recall that the digits consist of 1,797 samples with 64 features, where each of the 64 features is the brightness of one pixel in an 8×8 image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1fd891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e02164",
   "metadata": {},
   "source": [
    "We are going to load the data set from the sklean module and use the scale function to scale our data down. We want to convert the large values that are contained as features into a range between -1 and 1 to simplify calculations and make training easier and more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7436cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scale(digits.data)\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c073f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "clusters = kmeans.fit_predict(digits.data)\n",
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5079f1",
   "metadata": {},
   "source": [
    "The result is 10 clusters in 64 dimensions. Notice that the cluster centers themselves are 64-dimensional points, and can themselves be interpreted as the \"typical\" digit within the cluster. Let's see what these cluster centers look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a24c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(8, 3))\n",
    "centers = kmeans.cluster_centers_.reshape(10, 8, 8)\n",
    "for axi, center in zip(ax.flat, centers):\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a32850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(10):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = mode(digits.target[mask])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e85e8ab",
   "metadata": {},
   "source": [
    "Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(digits.target, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8cd161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e09962",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(digits.target, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bdf299",
   "metadata": {},
   "source": [
    "With just a simple k-means algorithm, we discovered the correct grouping for 80% of the input digits! Let's check the confusion matrix for this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43af02",
   "metadata": {},
   "source": [
    "The confusion matrix gives a lot of information about the model’s performance: \n",
    "\n",
    "As usual, the diagonal elements are the correctly predicted samples.\n",
    "\n",
    "#https://www.v7labs.com/blog/confusion-matrix-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9321ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b391001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(digits.target,labels)\n",
    "\n",
    "mat\n",
    "\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=digits.target_names,\n",
    "            yticklabels=digits.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8098d",
   "metadata": {},
   "source": [
    "# Visualizing the K-means clustering for handwritten data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64750771",
   "metadata": {},
   "source": [
    "\n",
    "Plotting the k-means cluster using the scatter function provided by the matplotlib module.\n",
    "Reducing the large dataset by using Principal Component Analysis (PCA) and fitting it to the previously defined k-means++ model.\n",
    "Plotting the clusters with different colors, a centroid was marked for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57004bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "kmeans_cluster = KMeans(init=\"k-means++\", n_clusters=10,\n",
    "                        n_init=10, random_state=0)\n",
    "  \n",
    "# Reducing the dataset\n",
    "pca = PCA(2)\n",
    "reduced_data = pca.fit_transform(digits.data)\n",
    "kmeans_cluster.fit(reduced_data)\n",
    "  \n",
    "# Calculating the centroids\n",
    "centroids = kmeans_cluster.cluster_centers_\n",
    "label = kmeans_cluster.fit_predict(reduced_data)\n",
    "unique_labels = np.unique(label)\n",
    "  \n",
    "# plotting the clusters:\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in unique_labels:\n",
    "    plt.scatter(reduced_data[label == i, 0],\n",
    "                reduced_data[label == i, 1],\n",
    "                label=i)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='k', zorder=10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "From the above graph, we can observe the clusters of the different digits are approximately separable from one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46240253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
